[config]
deployment_type = "app"
model = "qwen3-coder-30b-local"
fallback_models = ["qwen3-coder-30b-local"]
custom_model_max_tokens = 8192

[litellm_proxy]
custom_llm_provider = "openai"
api_base = "http://coder-30b-q4km-svc.llms.svc.cluster.local:8000"
#USE ENV VARIABLE FOR SECURITY TODO:fix
api_key = ""


[openai]
# key is still required structurally even if youâ€™re using a local endpoint
#key = ""
base_url = "http://coder-30b-q4km-svc.llms.svc.cluster.local:8000"
[github]
#TODO: load RSA webhook a batter way
app_id = 
webhook_secret =private_key = """-----BEGIN RSA PRIVATE KEY----- 
-----END RSA PRIVATE KEY-----
"""
