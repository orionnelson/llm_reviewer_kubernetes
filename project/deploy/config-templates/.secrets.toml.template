[config]
deployment_type = "app"
model = "qwen3-coder-30b-local"
fallback_models = ["qwen3-coder-30b-local"]
custom_model_max_tokens = 8192
ai_timeout=700 # Needed for R9 AI x370 cpu based ai


[litellm_proxy]
custom_llm_provider = "openai"
api_base = "http://coder-30b-q4km-svc.llms.svc.cluster.local:8000"
api_key = "$LITELLM_API_KEY"


[openai]
# key is still required structurally even if youâ€™re using a local endpoint
#key = ""
base_url = "http://coder-30b-q4km-svc.llms.svc.cluster.local:8000"
[github]
app_id = "$GITHUB_APP_ID"
webhook_secret ="$GITHUB_WEBHOOK_SECRET"
private_key = "$GITHUB_PRIVATE_KEY"
