apiVersion: apps/v1
kind: Deployment
metadata:
  name: coder-30b-q4km
  namespace: llms
spec:
  replicas: 1
  selector:
    matchLabels:
      app: coder-30b-q4km
  template:
    metadata:
      labels:
        app: coder-30b-q4km
    spec:
      containers:
        - name: llama-server
          image: ghcr.io/ggml-org/llama.cpp:server-cuda-b7549
          env:
           # Required: model path inside the container
            # Router mode: serve multiple models from a directory (do NOT set LLAMA_ARG_MODEL)
            - name: LLAMA_ARG_MODELS_DIR
              value: /models

            # Keep at most 2 models loaded at once (LRU eviction)
            - name: LLAMA_ARG_MODELS_MAX
              value: "2"
            # Context size â€“ keep sane on CPU
            - name: LLAMA_ARG_CTX_SIZE
              value: "40960"

            # Threads
            - name: LLAMA_ARG_THREADS
              value: "16"

            # Port inside container (llama-server default is 8080)
            - name: LLAMA_ARG_PORT
              value: "8080"

          ports:
            - name: http
              containerPort: 8080

          resources:
            requests:
              cpu: "8"
              memory: "16Gi"
            limits:
              cpu: "16"
              memory: "50Gi"

          volumeMounts:
            - name: models
              mountPath: /models
              readOnly: true

      volumes:
        - name: models
          hostPath:
            path: /srv/models/llama-router
            type: Directory
